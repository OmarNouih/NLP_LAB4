{"cells":[{"cell_type":"markdown","metadata":{},"source":["# GPT-2 and Fine-Tuning\n","\n","**GPT-2 Fine-Tuning for Text Generation**\n","\n","GPT-2, developed by OpenAI, is a powerful language generation model capable of generating coherent and contextually relevant text. Fine-tuning GPT-2 involves training the model on a specific dataset to adapt it to a particular task or domain. This process allows for the customization of the model's output to better suit the desired application, such as generating movie descriptions, product reviews, or creative writing."]},{"cell_type":"markdown","metadata":{},"source":["## Fine-tune the pre-trained model (GPT-2) to a customized dataset :\n","\n","The provided code demonstrates how to fine-tune a pre-trained GPT-2 model on a customized dataset. It first loads and prepares the data, then defines a custom dataset class to handle the text data. The dataset is split into training and validation sets, and training arguments are configured. Finally, the model is initialized and trained using the Trainer API from HuggingFace.\n"]},{"cell_type":"markdown","metadata":{},"source":["**Data Setup**: Import libraries and load the data titles and descriptions dataset."]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-21T17:43:19.349463Z","iopub.status.busy":"2024-05-21T17:43:19.349176Z","iopub.status.idle":"2024-05-21T17:43:26.227951Z","shell.execute_reply":"2024-05-21T17:43:26.227182Z","shell.execute_reply.started":"2024-05-21T17:43:19.349435Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, random_split\n","from transformers import GPT2Tokenizer, TrainingArguments, Trainer, GPT2LMHeadModel"]},{"cell_type":"markdown","metadata":{},"source":["**Model Preparation**: Initialize the GPT-2 tokenizer and model with medium-sized weights."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T17:43:27.236969Z","iopub.status.busy":"2024-05-21T17:43:27.236688Z","iopub.status.idle":"2024-05-21T17:44:30.914346Z","shell.execute_reply":"2024-05-21T17:44:30.913559Z","shell.execute_reply.started":"2024-05-21T17:43:27.236942Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"536a8c9b063d49e8bec842a005434665","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cc96ca7f144b423291c2341fe1495a49","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7658c084f85849d291692c51f8e16a89","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee69cb507995456887683fe745161943","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1ac899bef7e9404ca18f87e34b42c922","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/718 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8570c350a0e74ccc9cc779d2b3949be3","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["Embedding(50259, 1024)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium', bos_token='<|startoftext|>',eos_token='<|endoftext|>', pad_token='<|pad|>')\n","model = GPT2LMHeadModel.from_pretrained('gpt2-medium').cuda()\n","model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"markdown","metadata":{},"source":["**Dataset Creation**: Define a custom dataset class for data descriptions, tokenizing them and preparing input IDs and attention masks."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T17:44:30.916563Z","iopub.status.busy":"2024-05-21T17:44:30.916299Z","iopub.status.idle":"2024-05-21T17:44:31.029731Z","shell.execute_reply":"2024-05-21T17:44:31.028936Z","shell.execute_reply.started":"2024-05-21T17:44:30.916538Z"},"trusted":true},"outputs":[],"source":["descriptions = pd.read_csv('data_titles.csv')['description']"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T17:44:46.031210Z","iopub.status.busy":"2024-05-21T17:44:46.030816Z","iopub.status.idle":"2024-05-21T17:44:46.038215Z","shell.execute_reply":"2024-05-21T17:44:46.037380Z","shell.execute_reply.started":"2024-05-21T17:44:46.031172Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0    As her father nears the end of his life, filmm...\n","1    After crossing paths at a party, a Cape Town t...\n","2    To protect his family from a powerful drug lor...\n","Name: description, dtype: object"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["descriptions.head(3)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T17:44:31.031417Z","iopub.status.busy":"2024-05-21T17:44:31.031020Z","iopub.status.idle":"2024-05-21T17:44:35.017732Z","shell.execute_reply":"2024-05-21T17:44:35.016968Z","shell.execute_reply.started":"2024-05-21T17:44:31.031374Z"},"trusted":true},"outputs":[],"source":["max_length = max([len(tokenizer.encode(description)) for description in descriptions])"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T17:44:35.019137Z","iopub.status.busy":"2024-05-21T17:44:35.018854Z","iopub.status.idle":"2024-05-21T17:44:35.025922Z","shell.execute_reply":"2024-05-21T17:44:35.024988Z","shell.execute_reply.started":"2024-05-21T17:44:35.019099Z"},"trusted":true},"outputs":[],"source":["class dataDataset(Dataset):\n","    def __init__(self, txt_list, tokenizer, max_length):\n","        self.input_ids = []\n","        self.attn_masks = []\n","        self.labels = []\n","        for txt in txt_list:\n","            encodings_dict = tokenizer('<|startoftext|>' + txt + '<|endoftext|>', truncation=True,\n","                                       max_length=max_length, padding=\"max_length\")\n","            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n","            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return self.input_ids[idx], self.attn_masks[idx]"]},{"cell_type":"markdown","metadata":{},"source":["**Data Splitting**: Split the dataset into training and validation sets."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T17:44:49.638524Z","iopub.status.busy":"2024-05-21T17:44:49.638190Z","iopub.status.idle":"2024-05-21T17:44:52.694406Z","shell.execute_reply":"2024-05-21T17:44:52.693692Z","shell.execute_reply.started":"2024-05-21T17:44:49.638497Z"},"trusted":true},"outputs":[],"source":["dataset = dataDataset(descriptions, tokenizer, max_length=max_length)\n","train_size = int(0.9 * len(dataset))\n","train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])"]},{"cell_type":"markdown","metadata":{},"source":["**Training Configuration**: Set up training arguments for fine-tuning, specifying parameters like epochs, batch sizes, and logging settings.\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T17:45:04.279529Z","iopub.status.busy":"2024-05-21T17:45:04.279114Z","iopub.status.idle":"2024-05-21T17:45:04.286534Z","shell.execute_reply":"2024-05-21T17:45:04.285741Z","shell.execute_reply.started":"2024-05-21T17:45:04.279487Z"},"trusted":true},"outputs":[],"source":["training_args = TrainingArguments(output_dir='./results', num_train_epochs=1, logging_steps=100, save_steps=5000,\n","                                  per_device_train_batch_size=1, per_device_eval_batch_size=1,\n","                                  warmup_steps=10, weight_decay=0.05, logging_dir='./logs', report_to = 'none')\n"]},{"cell_type":"markdown","metadata":{},"source":["**Model Training**: Initialize the Trainer object and begin training the fine-tuned model."]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T17:45:07.158838Z","iopub.status.busy":"2024-05-21T17:45:07.158491Z","iopub.status.idle":"2024-05-21T18:08:59.811334Z","shell.execute_reply":"2024-05-21T18:08:59.810432Z","shell.execute_reply.started":"2024-05-21T17:45:07.158808Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='7926' max='7926' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [7926/7926 23:51, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>5.836100</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>1.961300</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>1.895700</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>1.951800</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>1.943900</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>1.807000</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>1.854600</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>1.917300</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>1.872200</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.778300</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>1.799700</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>1.793200</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>1.893900</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>1.825100</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.846300</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>1.815200</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>1.752900</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>1.837800</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>1.825200</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.818300</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>1.782000</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>1.794000</td>\n","    </tr>\n","    <tr>\n","      <td>2300</td>\n","      <td>1.875700</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>1.829900</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>1.729800</td>\n","    </tr>\n","    <tr>\n","      <td>2600</td>\n","      <td>1.773500</td>\n","    </tr>\n","    <tr>\n","      <td>2700</td>\n","      <td>1.845500</td>\n","    </tr>\n","    <tr>\n","      <td>2800</td>\n","      <td>1.788400</td>\n","    </tr>\n","    <tr>\n","      <td>2900</td>\n","      <td>1.774500</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>1.805200</td>\n","    </tr>\n","    <tr>\n","      <td>3100</td>\n","      <td>1.848200</td>\n","    </tr>\n","    <tr>\n","      <td>3200</td>\n","      <td>1.773800</td>\n","    </tr>\n","    <tr>\n","      <td>3300</td>\n","      <td>1.853300</td>\n","    </tr>\n","    <tr>\n","      <td>3400</td>\n","      <td>1.797800</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>1.772400</td>\n","    </tr>\n","    <tr>\n","      <td>3600</td>\n","      <td>1.748300</td>\n","    </tr>\n","    <tr>\n","      <td>3700</td>\n","      <td>1.794900</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>1.778900</td>\n","    </tr>\n","    <tr>\n","      <td>3900</td>\n","      <td>1.809400</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>1.800100</td>\n","    </tr>\n","    <tr>\n","      <td>4100</td>\n","      <td>1.752200</td>\n","    </tr>\n","    <tr>\n","      <td>4200</td>\n","      <td>1.733900</td>\n","    </tr>\n","    <tr>\n","      <td>4300</td>\n","      <td>1.749000</td>\n","    </tr>\n","    <tr>\n","      <td>4400</td>\n","      <td>1.791400</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>1.756800</td>\n","    </tr>\n","    <tr>\n","      <td>4600</td>\n","      <td>1.757000</td>\n","    </tr>\n","    <tr>\n","      <td>4700</td>\n","      <td>1.736900</td>\n","    </tr>\n","    <tr>\n","      <td>4800</td>\n","      <td>1.757100</td>\n","    </tr>\n","    <tr>\n","      <td>4900</td>\n","      <td>1.763900</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>1.768700</td>\n","    </tr>\n","    <tr>\n","      <td>5100</td>\n","      <td>1.774100</td>\n","    </tr>\n","    <tr>\n","      <td>5200</td>\n","      <td>1.729500</td>\n","    </tr>\n","    <tr>\n","      <td>5300</td>\n","      <td>1.767800</td>\n","    </tr>\n","    <tr>\n","      <td>5400</td>\n","      <td>1.756400</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>1.732800</td>\n","    </tr>\n","    <tr>\n","      <td>5600</td>\n","      <td>1.795100</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>1.740000</td>\n","    </tr>\n","    <tr>\n","      <td>5800</td>\n","      <td>1.746800</td>\n","    </tr>\n","    <tr>\n","      <td>5900</td>\n","      <td>1.723000</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>1.725800</td>\n","    </tr>\n","    <tr>\n","      <td>6100</td>\n","      <td>1.817000</td>\n","    </tr>\n","    <tr>\n","      <td>6200</td>\n","      <td>1.708600</td>\n","    </tr>\n","    <tr>\n","      <td>6300</td>\n","      <td>1.698000</td>\n","    </tr>\n","    <tr>\n","      <td>6400</td>\n","      <td>1.711100</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>1.716300</td>\n","    </tr>\n","    <tr>\n","      <td>6600</td>\n","      <td>1.768000</td>\n","    </tr>\n","    <tr>\n","      <td>6700</td>\n","      <td>1.699100</td>\n","    </tr>\n","    <tr>\n","      <td>6800</td>\n","      <td>1.743100</td>\n","    </tr>\n","    <tr>\n","      <td>6900</td>\n","      <td>1.747600</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>1.729500</td>\n","    </tr>\n","    <tr>\n","      <td>7100</td>\n","      <td>1.753400</td>\n","    </tr>\n","    <tr>\n","      <td>7200</td>\n","      <td>1.720700</td>\n","    </tr>\n","    <tr>\n","      <td>7300</td>\n","      <td>1.736900</td>\n","    </tr>\n","    <tr>\n","      <td>7400</td>\n","      <td>1.748300</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>1.716000</td>\n","    </tr>\n","    <tr>\n","      <td>7600</td>\n","      <td>1.706700</td>\n","    </tr>\n","    <tr>\n","      <td>7700</td>\n","      <td>1.733400</td>\n","    </tr>\n","    <tr>\n","      <td>7800</td>\n","      <td>1.709700</td>\n","    </tr>\n","    <tr>\n","      <td>7900</td>\n","      <td>1.668000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=7926, training_loss=1.8334816878051912, metrics={'train_runtime': 1431.9436, 'train_samples_per_second': 5.535, 'total_flos': 1046192214269952.0, 'epoch': 1.0, 'init_mem_cpu_alloc_delta': 58048, 'init_mem_gpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 18306, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 648694, 'train_mem_gpu_alloc_delta': 4257904640, 'train_mem_cpu_peaked_delta': 413258894, 'train_mem_gpu_peaked_delta': 617583616})"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["Trainer(model=model,  args=training_args, train_dataset=train_dataset, \n","        eval_dataset=val_dataset, data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n","                                                              'attention_mask': torch.stack([f[1] for f in data]),\n","                                                              'labels': torch.stack([f[0] for f in data])}).train()"]},{"cell_type":"markdown","metadata":{},"source":["## Generate a new paragraph according to a given sentence\n","\n","The code responds to this question by showcasing how to generate new text using the fine-tuned GPT-2 model. It prepares an input sentence, tokenizes it, and then generates new paragraphs based on the input. The generated text is decoded and displayed, providing new content based on the given sentence."]},{"cell_type":"markdown","metadata":{},"source":["**Input Preparation**: Prepare a starting sentence for text generation by tokenizing and converting it to tensor format."]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T18:08:59.814069Z","iopub.status.busy":"2024-05-21T18:08:59.813677Z","iopub.status.idle":"2024-05-21T18:08:59.818608Z","shell.execute_reply":"2024-05-21T18:08:59.817651Z","shell.execute_reply.started":"2024-05-21T18:08:59.814028Z"},"trusted":true},"outputs":[],"source":["generated = tokenizer(\"<|startoftext|> \", return_tensors=\"pt\").input_ids.cuda()"]},{"cell_type":"markdown","metadata":{},"source":["**Text Generation**: Generate new text based on the input sentence using the fine-tuned GPT-2 model."]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T18:08:59.819926Z","iopub.status.busy":"2024-05-21T18:08:59.819666Z","iopub.status.idle":"2024-05-21T18:09:09.328703Z","shell.execute_reply":"2024-05-21T18:09:09.327714Z","shell.execute_reply.started":"2024-05-21T18:08:59.819901Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]}],"source":["sample_outputs = model.generate(generated, do_sample=True, top_k=50, \n","                                max_length=300, top_p=0.95, temperature=1.9, num_return_sequences=20)\n"]},{"cell_type":"markdown","metadata":{},"source":["**Result Display**: Display the generated text sequences after decoding them from token IDs."]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T18:09:09.330554Z","iopub.status.busy":"2024-05-21T18:09:09.330172Z","iopub.status.idle":"2024-05-21T18:09:09.448413Z","shell.execute_reply":"2024-05-21T18:09:09.447611Z","shell.execute_reply.started":"2024-05-21T18:09:09.330511Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0:  ̶Curious fellow students from Bhola must navigate their very real identities and the highs and lows of high, campus life.\n","1:  ired with his wife for cheating in favor of his ex, a man is recruited by police investigating a group of other alleged criminals.\n","2:  ertie van bergangewandt! Dutertag de man terriessen and bohemians Vandus van Daekelu videns in action as two small family families travel to get their new holiday.\n","3:  ̶Elite members of India's underworld find their lives upended through the tragic actions of two politicians linked to a mafia underworld that spans three political parties.\n","4:  ”I love being a mom. Not everything she holds to is easy on a mother’s journey that ends at 45:00A local news program featuring guest actors.\n","5:   With every piece of furniture out-waste by a fortune.  When everyone becomes furniture, why aren't we buying everything? It only appears to have two problems in common household: 1) nobody needs nothing. Then every night of your life is the worst possible meal of all.\n","6:  ư‷️.¹ Mħᐜ�WhenThisExAsTwoDInWhileWhenAsAnInspAWhileWhenOnAWhile theAInFiveCAfterTwWhenInAWThreeAfterAfterAfterTElD-APAnThis episodeAfterFAdaptateCFollow theShTwoThis itemCeleABInspiral-ThreeDTwWMPIn theirThisJElInSanguTheTheTheThisInspInAfterSThreeAsAnOnThisFamilyPopA videoTwoWhenAfterEx-AnFriends'ACFollowWhile accusedAMFriendsInspated-A familyThisThree authorAsaThreeInIn orderAfter his holidayCelebrAfter recovering authorATwamedFriends,OnThe creators canDComimposed uponAn attorneyadoAnWhenAForberInspoppsAfter returning theSix members.Based byHFrom one,SRookie starsWhen Michael George's mother's best-InspiredAfterFollowing celebrities namedInspired friends of the family's son-the star of theDroneThe\n","7:  ’This epic fantasy based upon a series of fantasy novels centers around a single child born under extreme magical circumstances.\n","8:    by Hauptstewerd Wilhelm, Berlin is as one in this German horror-pirate story adaptation: humans at sea under their complete command have become enslaved and die for greed.\n","9:    with little magic in store or disarray?, anything but magic is up next for a group of four friends as they navigate teenage relationships as a magic world.\n","10:  ��� a year with them: 2 months of partying meets birthday time at one big, happy birthday party until the party's final day comes and goes without some drama.\n","11:  -Taken to school alone, young Yumi joins a boarding school party that transforms human teenage Tom into a powerful warrior.\n","12:  ired to kill themselves upon encountering an unexpected girl while at university to give them immortality, Nani is surprised. Is that all possible?\n","13:  iced up and overstimulated man Sam must battle his childhood friend to win the coveted GrandMaster crown held dear by all those they hoped they hadn would love.\n","14:  urn.com:user profile \"Jim C. Hendric\n","15:  ʁineb' – also know as H two-a little girlhood, or his-his word-tag-lacked version'spirator brother with a friend-brother who takes the bus with nois※-ing the guy, a-plus child’s-at-class act of mom’'s – that's more popular in Spain and at her nephew's popularity.\n","16:   During a global financial collapse, banksters seize control until a former director of a small department opens a restaurant on demand. She makes a fortune in two months.\n","17:  ōmashua is the last of Japan in the Five Nations, having been chosen and imprisoned. If he can keep his composure during one-on-ones, this war of the three countries and their countries seems on its way with doom to our island paradise.\n","18:  �This documentary follows two Indian men: their brother Rahul Singh with Delhi City, their mother Raja and a doctor who lives on their old Delhi Indian gated Community.\n","19:  icky Nicki gets dumped at home by a fellow actor – who also got Daddies dumped on for his wife. But the fallout is worse than planned. And Nicker doesn't seem to go by easily! It might take more than one chance.\n"]}],"source":["for i, sample_output in enumerate(sample_outputs):\n","    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":434238,"sourceId":2654038,"sourceType":"datasetVersion"}],"dockerImageVersionId":30086,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"nbformat":4,"nbformat_minor":4}
